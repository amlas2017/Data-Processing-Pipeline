---> Install Environnement de Travail avec PyCharm (jupyter-Lab inclu):     https://www.youtube.com/watch?v=Y1cKP_IJAgw


---> Les bonnes pratiques Spark:     --->   https://www.next-decision.fr/wiki/les-bonnes-pratiques-spark

    === Privilégier les RDD ou les DataFrames / Datasets en fonction des données et de la facilité d'utilisation
    === Choisir son format de fichiers idéal : Parquet avec une compression snappy.
    === Éviter les Shuffles de données et filtrer les informations non pertinentes : groupByKey(), reduceByKey(), repartition(), join()
    === Être attentif au partitionnement : coalesce() , répartition() et partitionBy().
    === Réutiliser efficacement des opérations - Persistance
    === Optimiser les performances avec des variables distribuées et partagées: 
        ----> brodcast() vous permet de distribuer une copie de la variable sur tous les nœuds du cluster.
        ----> Les accumulateurs sont des variables partagées sur plusieurs nœuds du cluster et supportent uniquement les opérations associatives et commutatives.
    


---> Aggregation de data avec spark utilisant "window" (java)  ----------> https://www.youtube.com/watch?v=WQgHKPCwr8A

---> Cours pratique  ------------CNAM------------>  http://cedric.cnam.fr/vertigo/Cours/RCP216/tpSparkScala.html

========================================================================================================================================================================

---> Lancer PySpark avec SparkSession:

import pyspark
from pyspark.sql import SQLContext, SparkSession
from pyspark import SQLContext

# Setup the Configuration
conf = pyspark.SparkConf()

spark = SparkSession.builder.config(conf=conf).getOrCreate()
sqlcontext = SQLContext(spark)


=========================================================================================================================================================================

TP Spark ----->   https://insatunisia.github.io/TP-BigData/tp2/

--->  Cours référence Spark (français)  ---> http://b3d.bdpedia.fr/spark-batch.html

---> What is SparkSession
SparkSession introduced in version Spark 2.0, It is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame and DataSet.
SparkSession’s object spark is default available in spark-shell and it can be created programmatically using SparkSession builder pattern.

---> Spark Session also includes all the APIs available in different contexts:
      Spark Context,
      SQL Context,
      Streaming Context,
      Hive Context.

==========================================================Lancer Spark Job avec Mysql dans un cluster AWS===============================================================================================================
   ------------->    https://www.youtube.com/watch?v=SpvMRarBXYA&list=PLcw5TTdQlsEBmYJ62aWNS6msW3A0qp4Wi&index=6
=======================================================================================================================================================================

---> Delat Lake  && Spark :   ---->     https://www.youtube.com/watch?v=ILu9r4v25jE&t=15s

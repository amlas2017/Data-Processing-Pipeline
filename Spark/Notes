---> Un RDD, venant de l’API bas-niveau, est une « boîte » destinée à contenir n’importe quel document, sans aucun préjugé sur la structure (ou l’absence de structure)
de ce dernier. Cela rend le système très généraliste, mais empêche une manipulation fine des constituants des documents, comme par exemple le filtrage en fonction de 
la valeur d’un champ. C’est le programmeur de l’application qui doit fournir la fonction effectuant le filtre.

On l’a dit, Spark implémente une API de plus haut niveau avec des structures assimilables à des tables relationnelles : les Dataset et DataFrame. Ils comportent un 
schéma, avec les définitions des colonnes. La connaissance de ce schéma – et éventuellement de leur type – permet à Spark de proposer des opérations plus fines, 
et des optimisations inspirées des techniques d’évaluation de requêtes dans les systèmes relationnels. En fait, on se ramène à une implantation distribuée du 
langage SQL. En interne, un avantage important de la connaissance du schéma est d’éviter de recourir à la sérialisation des objets Java (opération effectuée dans 
le cas des RDD pour écrire sur disque et échanger des données en réseau).

On distingue les Dataset, dont le type des colonnes est connu, et les DataFrames. Un DataFrame n’est rien d’autre qu’un Dataset (DataFrame = Dataset[Row]) contenant 
des lignes de type Row dont le schéma précis n’est pas connu. Ce typage des structures de données est lié au langage de programmation : Python et R étant dynamiquement
typés, ils n’accèdent qu’aux DataFrames. En Scala et Java en revanche, on utilise les Datasets, des objets JVM fortement typés.

Tout cela est un peu abstrait? Voici un exemple simple qui permet d’illustrer les principaux avantages des Dataset/DataFrame. Nous voulons appliquer un opérateur qui 
filtre les films dont le genre est « Drame ». On va exprimer le filtre (en simplifiant un peu) comme suit:

                         films.filter(film.getGenre() == 'Drame');

Si films est un RDD, Spark n’a aucune idée sur la structure des documents qu’il contient. Spark va donc instancier un objet Java (éventuellement en dé-sérialisant une 
chaîne d’octets reçue par réseau ou lue sur disque) et appeler la méthode getGenre(). Cela peut être long, et impose surtout de créer un objet pour un simple test.

Avec un Dataset ou DataFrame, le schéma est connu et Spark utilise son propre système d’encodage/décodage à la place de la sérialisation Java. De plus, dans le cas des Dataset, la valeur du champ genre peut être testée directement sans même effectuer de décodage depuis la représentation binaire.

Il est, en résumé, tout à fait préférable d’utiliser les Dataset dès que l’on a affaire à des données structurées.


---> Install Environnement de Travail avec PyCharm (jupyter-Lab inclu):     https://www.youtube.com/watch?v=Y1cKP_IJAgw


---> Les bonnes pratiques Spark:     --->   https://www.next-decision.fr/wiki/les-bonnes-pratiques-spark

    === Privilégier les RDD ou les DataFrames / Datasets en fonction des données et de la facilité d'utilisation
    === Choisir son format de fichiers idéal : Parquet avec une compression snappy.
    === Éviter les Shuffles de données et filtrer les informations non pertinentes : groupByKey(), reduceByKey(), repartition(), join()
    === Être attentif au partitionnement : coalesce() , répartition() et partitionBy().
    === Réutiliser efficacement des opérations - Persistance
    === Optimiser les performances avec des variables distribuées et partagées: 
        ----> brodcast() vous permet de distribuer une copie de la variable sur tous les nœuds du cluster.
        ----> Les accumulateurs sont des variables partagées sur plusieurs nœuds du cluster et supportent uniquement les opérations associatives et commutatives.
    


---> Aggregation de data avec spark utilisant "window" (java)  ----------> https://www.youtube.com/watch?v=WQgHKPCwr8A

---> Cours pratique  ------------CNAM------------>  http://cedric.cnam.fr/vertigo/Cours/RCP216/tpSparkScala.html

========================================================================================================================================================================

---> Lancer PySpark avec SparkSession:

import pyspark
from pyspark.sql import SQLContext, SparkSession
from pyspark import SQLContext

# Setup the Configuration
conf = pyspark.SparkConf()

spark = SparkSession.builder.config(conf=conf).getOrCreate()
sqlcontext = SQLContext(spark)


=========================================================================================================================================================================

TP Spark ----->   https://insatunisia.github.io/TP-BigData/tp2/

--->  Cours référence Spark (français)  ---> http://b3d.bdpedia.fr/spark-batch.html

---> What is SparkSession
SparkSession introduced in version Spark 2.0, It is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame and DataSet.
SparkSession’s object spark is default available in spark-shell and it can be created programmatically using SparkSession builder pattern.

---> Spark Session also includes all the APIs available in different contexts:
      Spark Context,
      SQL Context,
      Streaming Context,
      Hive Context.

==========================================================Lancer Spark Job avec Mysql dans un cluster AWS===============================================================================================================
   ------------->    https://www.youtube.com/watch?v=SpvMRarBXYA&list=PLcw5TTdQlsEBmYJ62aWNS6msW3A0qp4Wi&index=6
=======================================================================================================================================================================

---> Delat Lake  && Spark :   ---->     https://www.youtube.com/watch?v=ILu9r4v25jE&t=15s
